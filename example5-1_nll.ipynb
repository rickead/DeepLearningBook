{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Example 5.1 Modeling CSV data with Multilayer Perceptron Networks\n",
    "The first example from the Deep Learning book is modeling CSV data with a multilayer perceptron network (Patterson and Gibson. 175). This is entended to be a gentile introduction to the DL4J API using a simple model. My plan was to implement this exact model in TensorFlow using modern toolsets like Pandas, for loading data and Keras for creating, training and testing the model. I thought this would be the simpilest model to translate into TensorFlow and Keras, but I was wrong.\n",
    "\n",
    "The largest stumbling block in this transformation was the use of the Negative Log-Likelihood as the loss function. The log-likelihood is a function that is used in traditional pattern recognition to estimate parmaeters. \n",
    "\n",
    "The likelihood is the product of all of the data given the model parameters; e.g., \n",
    "\n",
    "$$L = \\prod_{k=1}^{N} p(x_k | \\Theta) $$\n",
    "\n",
    "Applying the negative log to the likehood, we get\n",
    "\n",
    "$$NLL = \\sum_{k=1}^{N} -\\ln p(x_k | \\Theta) $$\n",
    "\n",
    "where, $$p(x_k | \\Theta)$$ is the Gaussian probability of $$x_k$$ given the model parameters $$\\Theta$$\n",
    "\n",
    "The equation for the Gaussian probability is $$ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-1/2 ((x - \\mu)^2/\\sigma^2)}$$\n",
    "\n",
    "Applying the natural logarithm into the negative log likehood function, we have\n",
    "\n",
    "$$ NLL = \\sum_{k=1}^{N} \\frac{ln(2\\pi\\sigma^2)}{2} + \\frac{(x_k - \\mu)^2}{2\\sigma^2} $$\n",
    "\n",
    "If we assume that the observed values are samples from a Gaussian distribution with a predicted mean and variance, we can minimize the loss using the negative log-likehood criterion in place of the mean-squared error, with the following loss function, where $$y_k$$ is the true value and $$x_k$$ is the predicted value\n",
    "\n",
    "\n",
    "\n",
    "## Configure imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import tensorflow.python.platform\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "import wget  # pip3 install wget\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "matplotlib_loader = importlib.find_loader(\"matplotlib\")\n",
    "PLT_FOUND = matplotlib_loader is not None\n",
    "if PLT_FOUND:\n",
    "    import matplotlib as pyplot\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Example 5.1 with TensorFlow version: 2.3.0\nEager execution: True\n"
     ]
    }
   ]
  },
  {
   "source": [
    "The data used in this example is artifical, two parameter data of two different labels.\n",
    "\n",
    "We are going to read a few lines from one of the data files to determine how the data is organized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "path_prefix = os.path.join(\"data\", \"example1\")\n",
    "filenameTrain = \"saturn_data_train.csv\"\n",
    "filenameTest = \"saturn_data_eval.csv\"\n",
    "\n",
    "localFilenameTrain = os.path.join(path_prefix, filenameTrain)\n",
    "localFilenameTest = os.path.join(path_prefix, filenameTest)\n",
    "\n",
    "# Data by Dr. Jason Baldridge (http://www.jasonbaldridge.com) to test neural network frameworks.\n",
    "# Read \"https://github.com/jasonbaldridge/try-tf/tree/master/simdata\" and copy\n",
    "# to data/example1\n",
    "if (\n",
    "    not os.path.isdir(path_prefix)\n",
    "    or not os.path.exists(localFilenameTrain)\n",
    "    or not os.path.exists(localFilenameTest)\n",
    "):\n",
    "    # The actual URL for the raw data is:\n",
    "    URL = \"https://raw.githubusercontent.com/jasonbaldridge/try-tf/master/simdata/\"\n",
    "    print(\"Missing Saturn simulation data!\")\n",
    "    print(\"Downloading from\", URL)\n",
    "    os.mkdir(path_prefix)\n",
    "    wget.download(URL + \"/\" + filenameTrain, localFilenameTrain)\n",
    "    wget.download(URL + \"/\" + filenameTest, localFilenameTest)\n",
    "\n",
    "print(\"\\n\\nExample 5.1 with TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "print(\"\\nThe first five lines from the training data file:\")\n",
    "fd = open(localFilenameTrain)\n",
    "for i in range(5):\n",
    "    sys.stdout.write(fd.readline())\n",
    "fd.close()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1,-7.1239700674365,-5.05175898010314\n\n0,1.80771566423302,0.770505522143023\n\n1,8.43184823707231,-4.2287794074931\n\n0,0.451276074541732,0.669574142606103\n\n0,1.52519959303934,-0.953055551414968\n\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Here, we can see tha the file is arranged into three columns. The first column is the label of the two different groups of data (group 0 and group 1). The second column is are the two features. We will assume that these two features are simply the coordinates of the point that is in the labeled group; i.e., x and y. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 2\n",
    "\n",
    "def pack_features_vector(features, labels):\n",
    "    \"\"\"Pack the features into a single array.\"\"\"\n",
    "    features = tf.stack(list(features.values()), axis=1)\n",
    "    return features, labels\n",
    "\n",
    "def get_dataset(file_path, **kwargs):\n",
    "    \"\"\"Extract tf.data.Dataset representations of labels and features in CSV files given data in the format of label, feat[0], feat[1]. feat[2], etc..\n",
    "\n",
    "    Args:\n",
    "        file_path (string): The path to one or more CSV files to load.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset : A object that holds the (fetures, labels) data from the CSV file in batches.\n",
    "    \"\"\"\n",
    "    # Use the 'experimental' make_csv_dataset to load the input data from the CSV file\n",
    "    dataset = tf.data.experimental.make_csv_dataset(file_path, num_epochs=1, **kwargs)\n",
    "\n",
    "    # Pack the features from a map of tensorflow data itnoa single feature vector.\n",
    "    dataset = dataset.map(pack_features_vector)\n",
    "\n",
    "    # Convert the integer lables in the dataset to one-hot encoded values.\n",
    "    dataset = dataset.map(lambda x, y: (x, tf.one_hot(y, depth=NUM_LABELS)))\n",
    "    if PLT_FOUND:\n",
    "        pyplot.figure()\n",
    "        # There are only two labels in this dataset 0 or 1\n",
    "        idx = labels > 0.5\n",
    "        pyplot.scatter(feat[idx, 0], feat[idx, 1], marker=\"+\", c=\"#ff0000\")\n",
    "        idx = labels <= 0.5\n",
    "        pyplot.scatter(feat[idx, 0], feat[idx, 1], marker=\"o\", c=\"#00ff00\")\n",
    "        pyplot.show()\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 40  # Number of epochs, full passes of the data\n",
    "NUM_INPUTS = 2\n",
    "NUM_OUTPUTS = 2\n",
    "NUM_HIDDEN_NODES = 20\n",
    "MY_SEED = 123\n",
    "\n",
    "# Constants that specify the data to load from the .csv files.\n",
    "COLUMN_NAMES = [\"label\", \"x\", \"y\"]\n",
    "LABEL_NAME = COLUMN_NAMES[0]\n",
    "LABELS = [0, 1]\n",
    "\n",
    "\n",
    "# Load the training data set and test data set into batches and suffle the input data before use.\n",
    "training_batches = get_dataset(\n",
    "    localFilenameTrain,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    column_names=COLUMN_NAMES,\n",
    "    label_name=LABEL_NAME,\n",
    "    shuffle=True,\n",
    "    shuffle_seed=MY_SEED,\n",
    ")\n",
    "\n",
    "print(\"\\nDataset element defintion:\\n\\t\", training_batches.element_spec)\n",
    "\n",
    "testing_batches = get_dataset(\n",
    "    localFilenameTest,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    column_names=COLUMN_NAMES,\n",
    "    label_name=LABEL_NAME,\n",
    "    shuffle=True,\n",
    "    shuffle_seed=MY_SEED,\n",
    ")\n"
   ]
  },
  {
   "source": [
    "## Models\n",
    "Next, make the regression model to predict the label. For this example, the model has two layers. The input layer is an multilayer perceptron network with an RELU activation function and the output layer is is a softmax activation function with a negative log likelihood loss function. \n",
    "\n",
    "The weight initializer from the Deep Learning book is Xavier.\n",
    "\n",
    "\n",
    "## Loss functions\n",
    "Let's examine the negative log likelihood function again. \n",
    "\n",
    "$$ NLL = \\sum_{k=1}^{N} \\frac{ln(2\\pi\\sigma^2)}{2} + \\frac{(x_k - \\mu)^2}{2\\sigma^2} $$\n",
    "\n",
    "If we assume that the mean is 0.0 and the variance is 1.0, the negative log likelihood function simplifies to,\n",
    "\n",
    "$$ NLL = \\sum_{k=1}^{N} \\frac{ln(2\\pi)}{2} + \\frac{(x_k - \\mu)^2}{2} $$\n",
    "\n",
    "$$ NLL = \\frac{N ln(2\\pi)}{2} + \\sum_{k=1}^{N} \\frac{(x_k - \\mu)^2}{2} $$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(tf.keras.losses.Loss):\n",
    "    \"\"\"Custom loss function for calculating the loss as the mean-sequared error between the true output and the predicted output\"\"\"\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.reduce_mean(tf.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "class NegativeLogLikelihood(tf.keras.losses.Loss):\n",
    "    \"\"\"Custom loss function for calculating the loss as negative log likelihood between the true output and the predicted output\"\"\"\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.reduce_mean(tf.square(y_pred - y_true), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model. For this example, the model has two layers. The input layer is\n",
    "# an multilayer perceptron network with an RELU activation function and the output\n",
    "# layer is is a softmax activation function with a negative log likelihood loss function.\n",
    "#\n",
    "# The weight initializer in the Deep Learning book is Xavier and it is seeded with MY_SEED (123)\n",
    "initializer = tf.keras.initializers.GlorotNormal(seed=MY_SEED)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(\n",
    "            NUM_HIDDEN_NODES, activation=\"relu\", kernel_initializer=initializer\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            NUM_OUTPUTS, activation=\"softmax\", kernel_initializer=initializer\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Optimizer is Adam, loss function is mean squared error\n",
    "model.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nFit the training data.\")\n",
    "history = model.fit(training_batches, epochs=NUM_EPOCHS, verbose=1)\n",
    "model.summary()\n",
    "\n",
    "if PLT_FOUND:\n",
    "    # plot history\n",
    "    pyplot.plot(history.history[\"loss\"], label=\"loss\")\n",
    "    pyplot.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "    pyplot.title(\"Training loss and accuracy (MSE loss)\")\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "# Run against the test set. Final evaluation of the model\n",
    "scores = model.evaluate(testing_batches, verbose=0)\n",
    "print(\"Test set analysis accuracy: %.2f%%\" % (scores[1] * 100))\n"
   ]
  }
 ]
}